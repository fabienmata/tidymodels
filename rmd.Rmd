---
title: "Credit Risk with tidymodels"
author: "Fabien Mata"
date: "23/04/2021"
output: rmarkdown::github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1- The data 

### Context

The dataset contains 1000 entries with 10 categorical/symbolic attributes. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The link to the original dataset can be found [here](https://www.kaggle.com/kabure/german-credit-data-with-risk?select=german_credit_data.csv).

### Aim 

The goal of the study is for the bank to know whether a new customer is bad or good, based on the data that we have to decide if he should be given a loan or not.

### Content

Here are the variables :

1.  Age (numeric)

2.  Sex (text: male, female)

3.  Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)

4.  Housing (text: own, rent, or free)

5.  Saving accounts (text - little, moderate, quite rich, rich)

6.  Checking account (numeric, in DM - Deutsch Mark)

7.  Credit amount (numeric, in DM)

8.  Duration (numeric, in month)

9.  Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)

10. Risk (text : good, bad)

## 2. Packages

The work was done using the tidymodels framework. which is a collection of packages for modeling and machine learning using tidyverse principles. It is composed of the following packages :

-   'rsample' and 'recipes' for preprocessing.

-   'parsnip' and 'tune' for modeling and model optimization. Parsnip depends on several machine learning engines, it then requires the presence of each package for each engine.

-   'yardstick' for model evaluation and validation

'workfowsets' is not part of the tidymodels collection of package, even though it is a part of the **tidymodels** ecosystem. It helps to better manage machine learning workflows.

Several other packages were used during the exploratory analysis. Their roles are given in the comments.

```{r library, results='hide', message=FALSE, warning=FALSE}
library(tidymodels) 
library(naniar) #NA handling
library(finalfit) #NA handling
library(kableExtra) #pretty tables
library(workflowsets) 
###engines for parsnip models 
library(glmnet) #for regularised logistic 
library(rpart) #for decision tree
library(randomForest) #self explaining
library(klaR) #for discriminant analysis (engine)
library(discrim) #for discriminant analysis (function)
library(kknn) #for nearest neighbor
library(kernlab) #for support vector machine
library(themis)
```

## 3. Exporatory analysis

Let's take a first look at the data.

```{r import}
risk <- read.csv("https://raw.githubusercontent.com/fabienmata/tidymodels/master/data/german_credit_data.csv", 
                 row.names = 'X',
                 stringsAsFactors = TRUE)

risk %>% head() %>% kbl() %>% kable_styling()
```

Check column info :

```{r vars}
risk %>% str()
```

### a. Missing values handling

Number of missing values per column :

```{r naniar}
risk %>% gg_miss_var()
```

Missing data points position :

```{r finalfit}
risk %>% missing_plot()
```

### b. Descriptive analysis

Quantitative variables distribution :

```{r numerics }
nums = c("Age", "Credit.amount", "Duration", "Job")
risk[nums] %>%                    
  gather() %>%                            
  ggplot(aes(value)) +        
  facet_wrap(~ key, scales = "free") +  
  geom_density()
```

Correlation check between them

```{r corrplot}
require(corrplot)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
cor(risk[nums]) %>%  corrplot(method = "color", col = col(200),  
         type = "upper", order = "hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "darkblue", tl.srt = 45, #Text label color and rotation
         # Combine with significance level
         sig.level = 0.01,  
         # hide correlation coefficient on the principal diagonal
         diag = FALSE 
         )
```

Credit amount and the duration of the credits are somehow correlated. Let's visually check this link.

```{r credit amount vs duration}
risk %>% ggplot(aes(x= Duration, y = Credit.amount)) +
  geom_point(aes(colour = Risk), alpha=.3) + 
  theme(axis.text.x=element_blank(), legend.position = "none") +
  facet_wrap(~Risk)+
  labs(title = "Credit amount / duration link", x= "", y="")
```

The higher the amount, the longer the duration of the credit. There is no notable difference between bad and good customers. We can continue the analysis with this in mind.

Since the most interesting quantitative variables (credit amount, duration) are correlated, one of them would be enough to do the analysis of factor variables.

Before qualitative variable analysis, let's take a look to the age distribution between good and bad customers.

```{r credit amount}
risk %>% ggplot(aes(x = Age,fill = Risk))+ 
  geom_density( alpha=.5)+
  #facet_wrap(~Saving.accounts)+
  labs(title = "Age distribution", y= "", x="Age")
```

Qualitative variables analysis :

As said before, the most interesting variables to use for this is credit amount, it will be used as y.axis for each of the following figure.

-   Housing

```{r housing}
risk %>% ggplot(aes(x= Housing, y =Credit.amount, color = Risk)) + 
  geom_boxplot(size=1, alpha = .3) +
  scale_x_discrete() +
  scale_y_continuous()+ 
  geom_jitter(aes(color=Risk), alpha=.2)
```

No noticeable link.

-   Saving account

```{r saving accounts}
risk %>% ggplot(aes(x= Saving.accounts, y =Credit.amount, color = Risk)) + 
  geom_boxplot(size=1, alpha = .3) +
  scale_x_discrete() +
  scale_y_continuous()+ 
  geom_jitter(aes(color=Risk), alpha=.2)
```

-   Checking account

```{r checking account}
risk %>% ggplot(aes(x= Checking.account, y =Credit.amount, color = Risk)) +
  geom_boxplot(size=1, alpha = .3) +
  scale_x_discrete() +
  scale_y_continuous()+ 
  geom_jitter(aes(color=Risk), alpha=.2)
```

-   Purpose

```{r purpose}
risk %>% ggplot(aes(x= Purpose, y =Credit.amount, color = Risk)) + 
  geom_boxplot(size=1, alpha = .3) +
  scale_x_discrete() +
  scale_y_continuous()+ geom_jitter(aes(color=Risk), alpha=.2)

```

## 4. Prepocessing

### a. Split the data (rsample)

```{r split }
set.seed(1)
risk_split <- initial_split(risk,
                            prop = 0.75,
                            strata = Risk)

risk_training <- risk_split %>% 
  training()

risk_test <- risk_split %>% 
  testing()

#folds caracteristics for the cross validation 
set.seed(2)
risk_folds <- vfold_cv(data =  risk_training,
                       #number of partition
                       v = 5,
                       #outcome variable
                       strata = Risk)
```

### b. Feature engineering (recipes)

Here is a little description of each function and the reason for them:

-   step_relevel : used to change the level of the binary outcome variable. By default, when importing the data, the event level is set to 'bad'. We want the event to be 'good' for a better interpretability.

-   step_unknown : used to set na values to a new level in the affected factor variable.

-   step_normalize : self explanatory name.

-   step_dummy : one-hot encoding dropping the reference level.

-   step_smote : data augmentation for the outcome. In our outcome variable, the 'bad' level is underrepresented. The data is augmented so that there are the same number of entries for each levels in the training dataset before modeling.

```{r recipe}

risk_rec <- recipe(Risk ~., data = risk_training) %>% 
  #set the event/reference level to 'good'
  step_relevel(Risk, ref_level = 'good') %>% 
  #us the na's to create a new level 
  step_unknown(Saving.accounts, new_level = "no account") %>% 
  step_unknown(Checking.account, new_level = "no account") %>% 
  
  #normalize all numeric variables
  step_normalize(all_numeric()) %>% 
  
  #turn all the factors into dummies and delete the reference level
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_smote(Risk)
```

### c. Model specification (parsnip, tune)

For each model, the hyperparameters will be tuned using the 'tune' package and engines are declared to be used to fit the models.

-   an elastic net logistic regression (penalty and mixture stand for respectively L2 and L1 penalties)

-   a regularized discriminant analysis, the covariance is to be tuned so the model will neither be an LDA nor a QDA.

-   a decision tree.

-   a random forest, with mtry = number of sampled predictors.

-   a k nearest neighbor model.

-   a rbf kernel svm has been chosen as it is a good svm default model.

```{r models}
logit_tuned <- logistic_reg(penalty = tune(), 
                            mixture = tune()) %>%
  set_engine('glmnet') %>%
  set_mode('classification')

#regularised discriminant analysis 

rda_tuned <- discrim_regularized(frac_common_cov = tune(),
                                 frac_identity = tune()) %>% 
  set_engine('klaR') %>% 
  set_mode('classification')

#decision tree
dt_tuned <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')

#random forest 
rf_tuned <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
  set_engine('randomForest') %>%
  set_mode('classification')

#k nearest neighbors
knn_tuned <- nearest_neighbor(neighbors = tune(),
                              weight_func = tune(),
                              dist_power = tune()) %>% 
  set_engine('kknn') %>%
  set_mode('classification')

#support vector machine
svm_rbf_tuned <- svm_rbf(cost = tune(),
                          rbf_sigma = tune()) %>% 
  set_engine('kernlab') %>% 
  set_mode('classification')

```

## 5. Modeling

### a. Create a Workflowset

-   Worklowset parameters :

```{r workflowset parametrisation}
#make a list out of the models
models <- list(logit = logit_tuned,
               rda = rda_tuned,
               dt = dt_tuned, 
               rf = rf_tuned,
               knn = knn_tuned,
               svm = svm_rbf_tuned)

#incorporate them in a set of workflow
risk_wflow_set <- workflow_set(preproc = list(rec = risk_rec), 
                               models = models, 
                               cross = TRUE)  

#metrics we want for each model 
#we want : accuracy, sensitivity, specificity, area under the roc curve 
risk_metrics <- metric_set(accuracy, sens, spec, roc_auc)

```

-   Tune the models :

```{r tune grid}
wflow_set_grid_results <- risk_wflow_set %>% 
  workflow_map(
  #tune_grid() parameters
    resamples = risk_folds,
    grid = 10,
    metrics = risk_metrics,
  #workflow_map() own parameters
    seed = 3,
    verbose = TRUE
)
```

### b. Model screening

```{r rank table}
#rank the models by the area under the roc curve
library(kableExtra)
wflow_set_grid_results %>% 
  rank_results(rank_metric = "accuracy", select_best = TRUE) %>% 
  filter(.metric == "accuracy" | .metric == "sens" | .metric == "spec" ) %>% 
  kbl() %>% 
  kable_styling() %>% 
  scroll_box(width = "100%", height = "200px")
```

Tree based models have the best accuracies and sensitivities but low specificities. Logit and discriminant models have better specificities but lower performance overall. This can be confirmed by checking the area under the roc curve :

```{r rank plot}
#plot the performance of each model by rank
wflow_set_grid_results %>% 
  autoplot(rank_metric= "roc_auc", 
           metric = "roc_auc")
```

This confirms our intuition, let's take the best random forest model then.

### c. Finalize

Pull the best result's workflow and look at the hyperparameters :

-   mtry : The number of predictors that will be randomly sampled at each split when creating the tree models.

```{=html}
<!-- -->
```
-   trees : The number of trees contained in the ensemble.

-   min_n : The minimum number of data points in a node that are required for the node to be split further.

```{r collect best result}
#take the best result
best_results <- wflow_set_grid_results %>% 
  pull_workflow_set_result("rec_rf") %>% 
  select_best(metric = "roc_auc")

best_results
```

```{r fit best result}
#fit the best model
final_fit <- wflow_set_grid_results %>% 
  pull_workflow("rec_rf") %>% 
  finalize_workflow(best_results) %>% 
  last_fit(risk_split)
```

-   Best model metrics with test set

```{r best results metrics}
final_fit %>% collect_metrics()
```

-   Confusion matrix

```{r heatmap}
risk_predictions <- final_fit %>% collect_predictions()
conf_mat(risk_predictions,
         truth = Risk,
         estimate = .pred_class) %>%
  autoplot(type = 'heatmap')
```

Step_smote helped to has this better result, where we have a better sensitivity of the model and an acceptable specificity.

Finally, the roc curve :

```{r roc curve}
risk_predictions %>%
  roc_curve(truth = Risk, .pred_good) %>% 
  autoplot()
```
